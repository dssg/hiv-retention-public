{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import aequitas\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "from aequitas.plotting import Plot\n",
    "from aequitas.preprocessing import preprocess_input_df\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "dburl = os.environ['DBURL']\n",
    "engine = create_engine(dburl)\n",
    "pd.set_option('display.max_columns',100)\n",
    "path_to_models = '/gpfs/data/dsapp-lab/triage-production_runs_small/trained_models/'\n",
    "path_to_matrices = '/gpfs/data/dsapp-lab/triage-production_runs_small/matrices/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mgs_by_for = [21140, 21145, 21111]\n",
    "best_access_mg_id = 21144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgs_labels = {\n",
    "    21144: 'Random Forest (1000 trees, max depth=2)',\n",
    "        21140: \"Random Forest (10 trees, no max depth)\",\n",
    "        21145: 'Random Forest (5000 trees, max depth=2)',\n",
    "    21111: \"Decision Tree (no max depth)\",\n",
    "    '21144.0': 'Random Forest (1000 trees, max depth=2)',\n",
    "        '21140.0': \"Random Forest (10 trees, no max depth)\",\n",
    "        '21145.0': 'Random Forest (5000 trees, max depth=2)',\n",
    "    '21111.0': \"Decision Tree (no max depth)\",\n",
    "     'jeff_baseline': 'Current System'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group_to_use = 21111\n",
    "def get_demographics():\n",
    "    query = f\"\"\"with gender_race_info as (\n",
    "        select distinct entity_id, \n",
    "            race_id, race,\n",
    "            gender_id,\n",
    "            case when transmission_category like '%%MSM%%' then 'MSM' else 'other' end as trans_categ_msm\n",
    "        from features_cdph.demographics\n",
    "        join features_cdph.trans_categ using (entity_id)\n",
    "        join lookup_cdph.race using (race_id)\n",
    "        )\n",
    "        \n",
    "        select model_id, p.entity_id, p.as_of_date,\n",
    "                extract(year from p.as_of_date) as year,\n",
    "                label_value, \n",
    "                race, gender_id as gender, trans_categ_msm\n",
    "        from test_results.predictions p\n",
    "        left join gender_race_info using (entity_id)\n",
    "        join model_metadata.models using (model_id)\n",
    "        where model_group_id = {model_group_to_use}\n",
    "        \"\"\"\n",
    "    demo = pd.read_sql(query, engine)\n",
    "    return(demo)\n",
    "demo = get_demographics() # just demographics; combine with predictions later\n",
    "demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_same_mg(mg_id):\n",
    "    q = f\"\"\"\n",
    "        select model_id, train_end_time from model_metadata.models\n",
    "        where model_group_id = {mg_id}\n",
    "        \"\"\"\n",
    "    df = pd.read_sql(q, engine)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(m_id, baseline=False):\n",
    "    q = f\"\"\"\n",
    "        select model_id, entity_id, as_of_date,\n",
    "            score\n",
    "        from test_results.predictions p\n",
    "        where model_id ={m_id}\n",
    "        \"\"\"\n",
    "    df = pd.read_sql(q, engine)\n",
    "    if baseline:\n",
    "        # replace score with new score\n",
    "        q = f\"\"\"\n",
    "            with vl as (\n",
    "            select model_id, p.entity_id, as_of_date,\n",
    "                date_col, vl\n",
    "            from test_results.predictions p\n",
    "            join features_cdph.viral_load vl\n",
    "                on p.entity_id = vl.entity_id\n",
    "                and vl.date_col < p.as_of_date\n",
    "            where model_id ={m_id}\n",
    "            order by entity_id, as_of_date, date_col\n",
    "            )\n",
    "            select distinct model_id, entity_id, as_of_date,\n",
    "                last_value(vl) over (partition by entity_id, as_of_date) as latest_vl\n",
    "            from vl\n",
    "            order by as_of_date\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(q, engine)\n",
    "        df['score'] = df['latest_vl'].rank(pct=True)\n",
    "    return (df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_at_(m_id):\n",
    "    if 'bs' in str(m_id):\n",
    "        return({'value':None})\n",
    "    q = f\"\"\"\n",
    "        select value\n",
    "        from test_results.evaluations e\n",
    "        where metric='precision@' and parameter='1.0_pct'\n",
    "        and model_id={m_id}\n",
    "        \"\"\"\n",
    "    df = pd.read_sql(q, engine)\n",
    "    return(df.loc[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audition Waterfall Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comment = 'CDPH_12months_access'\n",
    "q = f'''\n",
    "    select distinct model_group_id from model_metadata.models\n",
    "    where model_comment = '{model_comment}'\n",
    "    '''\n",
    "model_groups = pd.read_sql(q, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_models_for = pd.read_csv('bias_p1.csv')\n",
    "access_models_for.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {'rank_pct':[0.01]}\n",
    "\n",
    "def get_for(m_id, pred):\n",
    "    for_race = []\n",
    "    # merge with demo\n",
    "    df = pd.merge(pred, demo, on=['entity_id', 'as_of_date'])\n",
    "    df = df.loc[df.groupby('entity_id')['label_value'].idxmax(), :]\n",
    "    df = df[[\"entity_id\", \"score\", \"label_value\", \"race\", \"gender\", \"trans_categ_msm\"]]\n",
    "    g = Group()\n",
    "    xtab, _ = g.get_crosstabs(df, thresholds)\n",
    "    for_black = xtab.loc[xtab.attribute_value=='Black / African American', 'for'].values[0]\n",
    "    for_white = xtab.loc[xtab.attribute_value=='White', 'for'].values[0]\n",
    "    for_race = for_black/for_white\n",
    "    \n",
    "    for_male =  xtab.loc[xtab.attribute_value=='male', 'for'].values[0]\n",
    "    for_female =  xtab.loc[xtab.attribute_value=='female', 'for'].values[0]\n",
    "    for_trans_f =  xtab.loc[xtab.attribute_value=='transgender female', 'for'].values[0]\n",
    "    \n",
    "    for_1 = xtab.loc[xtab.attribute_value=='MSM', 'for'].values[0]\n",
    "    for_2 = xtab.loc[xtab.attribute_value=='other', 'for'].values[0]\n",
    "    for_msm_v_other = for_1/for_2\n",
    "    return({'b_v_w':for_race, \n",
    "            'gender': [for_male, for_female, for_trans_f], \n",
    "            'trans_categ_msm': for_msm_v_other\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at models with lower disparity in FOR\n",
    "access_models_for = pd.DataFrame()\n",
    "#demo = get_demographics() # just demographics; combine with predictions later\n",
    "\n",
    "\n",
    "\n",
    "for mg_id in model_groups.model_group_id.values:\n",
    "    print(\"----------------------------\", mg_id)\n",
    "    for_race = []\n",
    "    for i, (m) in get_models_same_mg(mg_id).iterrows():\n",
    "        m_id = m['model_id']\n",
    "        pred = get_predictions(m_id)\n",
    "        for_all = get_for(m_id, pred)\n",
    "        p_at_10 = get_p_at_(m_id)\n",
    "        access_models_for = access_models_for.append({'model_group_id': mg_id, 'model_id': m_id,\n",
    "                            'train_end_time': m['train_end_time'],\n",
    "                            'p_at_10': p_at_10['value'],\n",
    "                            'for_b_v_w': for_all['b_v_w'],\n",
    "                            'for_m_v_f': for_all['gender'][0]/for_all['gender'][1],\n",
    "                            'for_trans_categ_msm': for_all['trans_categ_msm']\n",
    "                       }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_models_for['p_at_1']=access_models_for.model_id.apply(lambda mid: get_p_at_(mid)['value'])\n",
    "access_models_for.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get baseline\n",
    "print(\"Jeff baseline\")\n",
    "for i, (m) in get_models_same_mg(best_access_mg_id).iterrows():\n",
    "    m_id = m['model_id']\n",
    "    pred = get_predictions(m_id, baseline=True)\n",
    "    for_all = get_for(m_id, pred)\n",
    "    p_at_10 = get_p_at_(m_id)\n",
    "    pred['rank_pct'] = pred.score.rank(pct=True)\n",
    "    pred['outcome'] = pred.rank_pct.rank(pct=True)\n",
    "    p = pd.merge(pred, demo, on=['entity_id', 'as_of_date'])\n",
    "    top = p[p.rank_pct>0.99]\n",
    "    p_at_1 = top.label_value.sum()/top.label_value.count()\n",
    "    access_models_for = access_models_for.append({'model_group_id': 'jeff_baseline', 'model_id': 'bs'+str(m['model_id']),\n",
    "                            'train_end_time': m['train_end_time'],\n",
    "                            'p_at_10': p_at_10['value'],\n",
    "                            'p_at_1': p_at_1,\n",
    "                            'for_b_v_w': for_all['b_v_w'],\n",
    "                            'for_m_v_f': for_all['gender'][0]/for_all['gender'][1],\n",
    "                            'for_trans_categ_msm': for_all['trans_categ_msm']\n",
    "                       }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access_models_for.to_csv('bias_p1.csv', index=False)\n",
    "access_models_for.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_models_for = access_models_for.drop('distance', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_from_best(disparity_col):\n",
    "    # best = 1 = no disparity\n",
    "    n_times = access_models_for.train_end_time.nunique()\n",
    "    # is this the best way of computing distance?\n",
    "    d_col = 'distance_'+disparity_col\n",
    "    if d_col not in access_models_for.columns:\n",
    "        access_models_for['distance_'+disparity_col] = abs(access_models_for[disparity_col]-1)\n",
    "\n",
    "    # x = distance\n",
    "    # y = number of times\n",
    "    # each line is a model group\n",
    "    df_dist_from_best = pd.DataFrame()\n",
    "    for distance in np.arange(0,1,0.01):\n",
    "        for mg_id, d in access_models_for.groupby('model_group_id'):\n",
    "            df_dist_from_best = df_dist_from_best.append({'model_group_id': mg_id, \n",
    "                            'distance': distance,\n",
    "                            'pct_of_time': d[d[d_col] < distance].shape[0]/n_times,\n",
    "                       }, ignore_index=True)    \n",
    "    return(df_dist_from_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_from_best = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for disparity_col in ['for_b_v_w', 'for_m_v_f', 'for_trans_categ_msm']:\n",
    "    if disparity_col not in dist_from_best:\n",
    "        print(\"calculating distance from best for \"+disparity_col+\" ...\")\n",
    "        dist_from_best[disparity_col] = get_distance_from_best(disparity_col)\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(1,figsize=(24, 10))\n",
    "    sns.set_context(\"poster\", font_scale=1, rc={\"lines.linewidth\": 2,\"lines.markersize\":12})\n",
    "\n",
    "    for mg_id, d in dist_from_best[disparity_col].groupby('model_group_id'):\n",
    "        if mg_id in ['21140.0', '21145.0', '21111.0', \"jeff_baseline\"]:\n",
    "            _ = plt.plot(d.distance, d.pct_of_time, \n",
    "                 label=mg_id, marker='', linestyle='-', linewidth=10)\n",
    "        else:\n",
    "            _ = plt.plot(d.distance, d.pct_of_time, \n",
    "                 label=mg_id, marker='', linestyle='-')\n",
    "\n",
    "    _ = plt.xlabel(\"Distance from 1 for \"+ disparity_col, fontsize=24)\n",
    "    _ = plt.ylabel(\"Percentage of time\", fontsize=24)\n",
    "    _ = plt.legend(ncol=4) #bbox_to_anchor=(0., .85, 1., .102), loc='upper center', ncol=2, borderaxespad=0., fontsize=24)\n",
    "    plt.show()\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they basically reach 1 at similar points\n",
    "dist_from_best[disparity_col].groupby('model_group_id').apply(lambda d: d[d.pct_of_time<1].tail(1).distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'precision@'\n",
    "parameter = '1.0_pct'\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, ax = plt.subplots(1,figsize=(24, 10))\n",
    "sns.set_context(\"poster\", font_scale=1, rc={\"lines.linewidth\": 4,\"lines.markersize\":12})\n",
    "#_ = plt.ylim(0,1)\n",
    "_ = plt.ylabel(f'{metric}:{parameter}')\n",
    "\n",
    "for model_group_id in best_mgs_by_for + [best_access_mg_id, 'jeff_baseline']:\n",
    "    d = access_models_for[access_models_for.model_group_id == str(model_group_id)+\".0\"].sort_values('train_end_time')\n",
    "    _ = plt.plot(d.train_end_time, d.for_b_v_w, label=mgs_labels[model_group_id],\n",
    "                 marker='o',linestyle='-', alpha=0.8)\n",
    "\n",
    "_ = plt.ylabel(r\"$\\frac{FOR_{Black}}{FOR_{White}}$\", fontsize=24)\n",
    "_ = plt.xlabel(\"Year of Appointment for Validation Cohort\", fontsize=24)\n",
    "_ = plt.ylim(0.5, 1.8)    \n",
    "_ = ax.axhspan(0.9, 1.1, alpha=0.3, color='#6A659999')\n",
    "   \n",
    "_ = plt.legend() #bbox_to_anchor=(0., .85, 1., .102), loc='upper center', ncol=2, borderaxespad=0., fontsize=24)\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgs_to_plot = [str(x)+\".0\" for x in best_mgs_by_for+[best_access_mg_id]]+['jeff_baseline']\n",
    "mgs_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = access_models_for[access_models_for.model_group_id.isin(mgs_to_plot)]\n",
    "disp.model_group_id.unique()\n",
    "disp = disp.groupby('model_group_id')[['for_b_v_w', 'p_at_1']].describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "disp.columns = ['_'.join(col).strip() for col in disp.columns.values]\n",
    "disp\n",
    "disp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(plt, d, label):\n",
    "    #r = d.for_max - d.for_min\n",
    "    _ = plt.plot(d.p_at_1_mean, d.for_b_v_w_mean, marker='o', linestyle='', label=label)\n",
    "    c = _[0].get_color()\n",
    "    _ = plt.errorbar(d.p_at_1_mean, d.for_b_v_w_mean, \n",
    "                 xerr=[d.p_at_1_mean-d['p_at_1_25%'],d['p_at_1_75%']-d.p_at_1_mean], \n",
    "                 marker='', linestyle='', color=c, linewidth=10)\n",
    "    _ = plt.errorbar(d.p_at_1_mean, d.for_b_v_w_mean, \n",
    "                 xerr=[d.p_at_1_mean-d['p_at_1_5%'],d['p_at_1_95%']-d.p_at_1_mean], \n",
    "                 marker='', linestyle='', color=c)\n",
    "    _ = plt.errorbar(d.p_at_1_mean, d.for_b_v_w_mean, \n",
    "                 yerr=[d.for_b_v_w_mean-d['for_b_v_w_25%'],d['for_b_v_w_75%']-d.for_b_v_w_mean], \n",
    "                 marker='', linestyle='', color=c, linewidth=10)\n",
    "    _ = plt.errorbar(d.p_at_1_mean, d.for_b_v_w_mean, \n",
    "                 yerr=[d.for_b_v_w_mean-d['for_b_v_w_5%'],d['for_b_v_w_95%']-d.for_b_v_w_mean], \n",
    "                 marker='', linestyle='', color=c)\n",
    "    return(plt)\n",
    "\n",
    "sns.set_style('whitegrid')                                                                                                                                      \n",
    "sns.set_context(\"poster\", font_scale=1, rc={\"lines.linewidth\": 2.25,\"lines.markersize\":20})                                                                  \n",
    "fig, ax = plt.subplots(1,1,figsize=(24,10))\n",
    "\n",
    "for i, row in disp.groupby('model_group_id'):\n",
    "    plt = plot_stuff(plt, row, mgs_labels[i])\n",
    "_ = ax.axhspan(0.9, 1.1, alpha=0.3, color='#6A659999')\n",
    "_ = plt.ylabel(r'$\\frac{FOR_{Black}}{FOR_{White}}$', fontsize=40)\n",
    "_ = plt.xlabel(\"Average Positive Predictive Value for top 1%\")\n",
    "_ = plt.ylim(0.5, 1.8)\n",
    "h, l = ax.get_legend_handles_labels()\n",
    "new_h = []\n",
    "new_l = []\n",
    "for i in range(0, len(h)):\n",
    "    if 'mean' in l[i]:\n",
    "        continue\n",
    "    new_h.append(h[i])\n",
    "    new_l.append(l[i])\n",
    "_ = plt.legend(new_h, new_l, ncol=2, loc=\"upper center\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
